---
title: "MA 3: Portfolio estimation"
author: "tgx333 \\ lhb642 \\ "
date: "13/6/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Loading packages:
library(RSQLite)
library(tidyverse)
library(tidymodels) 
library(furrr) 
library(glmnet)
library(broom)
library(timetk)
library(scales)
library(lubridate)
library(scales)
library(frenchdata)
library(caret)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(mltools)
library(data.table)
library(vtable)
library(keras)
library(quadprog)
library(rmgarch)
library(xts)
```

##1.

We start by loading the CRSP data set. To keep the analysis simple we work with a balanced panel and exclude tickers, that are not traded for the full period. We end up with a tradable universe of 731 stocks.  
```{r}
# Select all stocks from the CRSP universe
 tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite",
                           extended_types = TRUE)

# Reading in crsp_monthly dataset
crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>%
  select(permno,month,ret_excess,industry) %>%
  collect() %>%
  mutate(month = as.Date(as.POSIXct.Date(month)))

# Clean data by removing stocks, which are not traded on every single trading day thourhout the whole sample period. 
data <- crsp_monthly %>%
  group_by(permno) %>%
  mutate(n=n()) %>%
  ungroup() %>%
  filter(n == max(n)) %>%
  select(-n)

remove(crsp_monthly)
```

**Provide a brief summary of the sample: How many stocks does your investment universe consist of?**

We now turn to produce the summary statistics. In conclusion, we are working with a dataset consisting of 107 stocks running for 731 months between February 1st 1960 and December 1st 2020. Moreover, some statistics about the annualized excess returns are provided, all in percent. 
```{r, fig.align = 'center'}
# Provide a brief summary of the sample:

data %>% 
  summarise(
        'N permnos' = n_distinct(permno),
        "N months"  = n_distinct(month),
        'Start date' = min(month),
        'End date' = max(month),
        Mean  = 12*mean(100*ret_excess),
        'SD' = sqrt(12) * sd(100 * ret_excess),
        Min = min(100*ret_excess),
        Max = max(100*ret_excess)
         ) %>%
 #mutate(Mean = format(Mean, digits = 2), SD = format(SD, digits = 2),
  #      Min = format(Min, digits = 3), Max = format(Max, digits = 3)) %>%
  kbl(caption = "Summary statistics for the tradable universe and for annualized excess returns (%)",digits = 1) %>%
  kable_classic(full_width = F, html_font = "Cambria")


remove(table)

```
**Derive a closed-form solution for the mean-variance efficient portfolio $\omega^*_{t+1}$ based on the transaction cost specification above.**

We have our initial function:
$$
\omega^*_{t+1} := arg\ max\ \omega^{'} \mu - v_t(\omega, \omega_{t+}, \beta) - (\gamma/2)\omega^{'} \Sigma \omega 
$$
Subbing the formula for transaction costs, $TC(\omega,\omega_{t+})$ we get:
$$
\omega^*_{t+1} := arg\ max\ \omega^{'} \mu - \lambda(\omega-\omega_{t+})^{'}\Sigma(\omega-\omega_{t+})- (\gamma/2)\omega^{'} \Sigma \omega, 
$$
which we can rewrite:
$$
\omega^*_{t+1} := arg\ max\ \omega^{'} \mu - \lambda(\omega^{'}\Sigma\omega - \omega^{'}\Sigma\omega_{t+}-\omega^{'}_{t+}\Sigma\omega + \omega^{'}_{t+}\Sigma\omega_{t+}) - (\gamma/2)\omega^{'} \Sigma \omega 
$$
$$
= arg\ max\ \omega^{'} \mu - \lambda(\omega^{'}\Sigma\omega - 2\omega^{'}\Sigma\omega_{t+} + \omega^{'}_{t+}\Sigma\omega_{t+}) - (\gamma/2)\omega^{'} \Sigma \omega
$$
$$
= arg\ max\ \omega^{'} \mu - \lambda  \Sigma(\omega^{'}\omega - 2\omega^{'}\omega_{t+} + \omega^{'}_{t+}\omega_{t+}) - (\gamma/2)\omega^{'} \Sigma \omega. 
$$
As we are maximizing with respect to $\omega$ we can set the last part equal to 0, as this does not depend on $\omega$. This yields:
$$
\omega^*_{t+1} := arg\ max\ \omega^{'} \mu - \lambda  \Sigma(\omega^{'}\omega - 2\omega^{'}\omega_{t+}) - (\gamma/2)\omega^{'} \Sigma \omega. 
$$
Rewriting the above equation we get:
$$
\omega^*_{t+1} := arg\ max\ \omega^{'}(\mu - 2\lambda\Sigma\omega_{t+}) - (\gamma/2)\omega^{'}(\Sigma + ((2\lambda\Sigma)/\lambda))\omega
$$
$$
= arg\ max\ \omega^{'}\mu^{*} - (\gamma/2)\omega^{'}\Sigma^{*}\omega
$$

where $\mu - 2\lambda\Sigma\omega_{t+} = \mu^{*}$ and $\Sigma + ((2\lambda\Sigma)/\lambda) = \Sigma^{*}$. As this is on the same form as the maximization problem without transaction costs, we find the following solution:
$$
\omega^*_{t+1} = (1/\gamma)(\Sigma^{*-1}-(1/(i'\Sigma^{*-1}i))\Sigma^{*-1}ii'\Sigma^{*-1})\mu^{*}+(1/(i'\Sigma^{*-1}i))\Sigma^{*-1}i
$$
**Discuss if the assumption of transaction costs proportional to volatility makes sense.**

To find the optimal weight we incorporate the 'compute_efficient_weights' function from chapter 11.4 in (Scheuss et. al 2022), where $\mu$ denotes the historical mean return, sigma, $\Sigma$ is the covariance matrix, gamma, $\gamma$, is the coefficient of risk aversion (4 by pre-specification), lambda, $\lambda$ is the transaction costs and w_prev is the weight before re-balancing. The function calculates the efficient portfolio weight in general form, while allowing for transaction costs. The transaction costs are conditional on the holdings before reallocation. The figure shows that if lambda (tr. cost) is zero, the portfolio allocation resembles the standard mean-variance efficient framework, where the aim is to maximize the sharpe ratio.\\ 

```{r}
# Function to compute optimal portfolio weights based on inputs. 

efficient_weights <- function(mu, sigma, gamma = 4, lambda, w_prev) {
  
  iota <- rep(1, ncol(sigma))
  
  sigma_adjusted <- sigma + lambda/gamma * diag(ncol(sigma))
  mu_adjusted <- mu + lambda*w_prev
  
  sigma_inv <- solve(sigma_adjusted)
  
  w_mvp <- sigma_inv %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  
  w_opt <- w_mvp + 1/gamma*(sigma_inv - (sigma_inv %*% iota %*% t(iota) %*% sigma_inv)/sum(sigma_inv)) %*% mu_adjusted
  return(as.vector(w_opt))
}

```


The first step is to calculate the returns matrix, where the entire series of returns for each stock is obtained. From this it is possible to calculate the historical mean, $\mu$ and covariance matrix, $\Sigma$.
```{r}
# We will need a a (T x N) matrix of returns to compute a matrix of returns: 
ret_matrix <- data %>%
  select(-industry) %>%
  pivot_wider(
    names_from = permno,
    values_from = ret_excess
  ) %>% 
  select(-month)


# using the returns matrix, we can compute the covariance matrix 
sigma <- cov(ret_matrix)
mu <- colMeans(ret_matrix) # extract the sample mean returns from the return matrix

remove(data)

```

Next we perform Ledoit-Wolf shrinkage on the variance-covariance matrix. This comes in handy, as the method serves as an effective way of overcoming issues related to var-covar matrices in large dimensions. The function is rather complex, why we refer the reader to chapter 9.2 in "Exercises for Advanced Empirical Finance: Topics and Data Science" by (Scheuss et. al 2022) for a deeper explanation.
```{r}
# Ledoit-Wolf estimation
compute_ledoit_wolf <- function(x) {
  # Computes Ledoit-Wolf shrinkage covariance estimator
  # This function generates the Ledoit-Wolf covariance estimator  as proposed in Ledoit, Wolf 2004 (Honey, I shrunk the sample covariance matrix.)
  # X is a (t x n) matrix of returns
  t <- nrow(x)
  n <- ncol(x)
  x <- apply(x, 2, function(x) if (is.numeric(x)) # demean x
    x - mean(x) else x)
  sample <- (1/t) * (t(x) %*% x)
  var <- diag(sample)
  sqrtvar <- sqrt(var)
  rBar <- (sum(sum(sample/(sqrtvar %*% t(sqrtvar)))) - n)/(n * (n - 1))
  prior <- rBar * sqrtvar %*% t(sqrtvar)
  diag(prior) <- var
  y <- x^2
  phiMat <- t(y) %*% y/t - 2 * (t(x) %*% x) * sample/t + sample^2
  phi <- sum(phiMat)

  repmat = function(X, m, n) {
    X <- as.matrix(X)
    mx = dim(X)[1]
    nx = dim(X)[2]
    matrix(t(matrix(X, mx, nx * n)), mx * m, nx * n, byrow = T)
  }

  term1 <- (t(x^3) %*% x)/t
  help <- t(x) %*% x/t
  helpDiag <- diag(help)
  term2 <- repmat(helpDiag, 1, n) * sample
  term3 <- help * repmat(var, 1, n)
  term4 <- repmat(var, 1, n) * sample
  thetaMat <- term1 - term2 - term3 + term4
  diag(thetaMat) <- 0
  rho <- sum(diag(phiMat)) + rBar * sum(sum(((1/sqrtvar) %*% t(sqrtvar)) * thetaMat))

  gamma <- sum(diag(t(sample - prior) %*% (sample - prior)))
  kappa <- (phi - rho)/gamma
  shrinkage <- max(0, min(1, kappa/t))
  if (is.nan(shrinkage))
    shrinkage <- 1
  sigma <- shrinkage * prior + (1 - shrinkage) * sample
  return(sigma)
}


```

To calculate the optimal portfolio with 1/N initial we will use the hard-coded weight parameter from the function, as this provides an initially equally-weighted portfolio. \ 

```{r, fig.width=8, fig.height=4}
# Creating the initial naive weight
naive_weights = 1/ncol(sigma) * rep(1,ncol(sigma))

w_efficient = efficient_weights(sigma = sigma,
                                   mu = mu,
                                   lambda = 0,
                                   w_prev = naive_weights)

transactions_costs <- expand_grid(lambda = 20 * qexp((1:99)/100)) %>%
  mutate(weights = map(.x = lambda,
                       ~efficient_weights(
                         sigma = sigma,
                         mu = mu,
                         lambda = .x / 10000,
                         w_prev = naive_weights
                         
                       )
                       ),
         concentration = map_dbl(weights, ~sum(abs(. - w_efficient)))
         )

transactions_costs %>% 
  ggplot(aes(x = lambda, y = concentration)) + 
  geom_line(color="tomato3") +
  labs(x = "Transaction cost parameter", 
       y = "Distance from efficient weights",
       title = "Distance from efficient portfolio weights for different transaction cost")

remove(transactions_costs, mu, sigma)

```


The higher the level of transaction costs, the closer will the optimal portfolio will be to the equally-weighted. Turnover penalization will make the investor reluctant towards making trades, as it is costly, and therefore choose appropriate portfolio-weights. This also makes sense as increasing transaction costs implies a higher demanded return - at some point, the costs just will not outweight the gains from the transactions, why investors will be reluctant to rebalance their portfolio. 


 

With rolling window forecasts, we recursively re-estimate the model and store the one ahead covariance forecasts. We implement the strategy in the following way. For the backtesting, we recompute optimal weight based purely on past available data. We use a window length of 150 periods which gives us 333 periods of performance. We test three strategies: A transaction cost-adjusted portfolio (MV (TC)), a mean-variance portfolio without short-selling, but with transaction costs (MV (No SS)) and an equally weighted naive portfolio (Naive). Initial weights are set as a equally weighted portfolio and we evaluate the portfolios through the mean returns nets of transaction costs, std. deviation of returns net of transaction costs, the sharpe ratio and the portfolio turnover.    

```{r}
# recomputing optimal weights

window_length <- 150
periods <-  nrow(ret_matrix) - window_length
lambda = 200/10000

performance_values <- matrix(NA,
                             nrow = periods,
                             ncol = 3) 

colnames(performance_values) = c("raw_return","turnover","net_return")


performance_values <- list("MV (TC)" = performance_values,
                           "MV (No SS)" = performance_values, 
                            "Naive" = performance_values)

w_prev_1 <- w_prev_2 <- w_prev_3 <- naive_weights # Initial weights of all portfolios

# Helper function to adjust weights due to returns changing
adjust_weights <- function(w, next_return){
  w_prev <- 1 + w * next_return
  as.numeric(w_prev / sum(as.vector(w_prev)))
}

# Helper function to calculate performance evaluation: compute realized returns net of transaction costs.
evaluate_performance <- function(w, w_previous, next_return, lambda){
  raw_return <- as.matrix(next_return) %*% w
  turnover <- sum(abs(w - w_previous))
  net_return <- raw_return - lambda * turnover
  c(raw_return, turnover, net_return)
}

```

With the helper functions, we are ready to perform the rolling-window estimation. This is done using a for loop through all the periods. Lastly, the performance is evaluated through the summarized key figures.
```{r}
# The following code chunk performs rolling-window estimation:

for (p in 1:periods) {
  
  returns_window <- ret_matrix[p : (p + window_length - 1), ] # window of return rows. window of rows and all columns
  one_ahead_return <- ret_matrix[p + window_length,] # Next periods return
  

  mu  <- colMeans(returns_window)
  sigma_lw <- compute_ledoit_wolf(returns_window)
  
  # Mean-variance transaction cost-adjusted portfolio:

  w_MV_TC <- efficient_weights(mu = mu,
                                    sigma = sigma_lw,
                                    lambda = lambda,
                                    w_prev = w_prev_1 # first naive, then adjusted.
                                    )
  
  performance_values[[1]][p,] <- evaluate_performance(w = w_MV_TC,
                                                      w_previous = w_prev_1,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda
                                                      )
  
  w_prev_1 = adjust_weights(w_MV_TC, one_ahead_return)
  

 # Mean-variance without short-selling portfolio (still with transaction costs):
  
  w_MV_NSS <- solve.QP(Dmat = 4 * sigma_lw,
                       dvec = mu,
                       Amat = cbind(1,diag(ncol(returns_window))),
                       bvec = c(1,rep(0, ncol(returns_window))),
                       meq = 1
                       )$solution
  
  
  performance_values[[2]][p,] <- evaluate_performance(w = w_MV_NSS,
                                                      w_previous = w_prev_2,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda
                                                      ) 
  
w_prev_2 = adjust_weights(w_MV_NSS, one_ahead_return)

  
  # Naive portfolio allocation:
  
  performance_values[[3]][p,] <- evaluate_performance(w = naive_weights,
                                                      w_previous = w_prev_3,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda)
  
  w_prev_3 = adjust_weights(naive_weights, one_ahead_return)

}


performance <- lapply(performance_values, as_tibble) %>% 
  bind_rows(.id = "strategy")
  
performance %>%
  group_by(strategy) %>%
  summarise(
    Mean = 12 * mean(100 * raw_return),
    SD = sqrt(12) * sd(100 * raw_return),
    'Sharpe Ratio' = if_else(Mean > 0, Mean / SD, NA_real_),
    Turnover = mean(turnover)
  ) %>% kbl(caption = "Summary of portfolio performance",digits = 2) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


As presented in the above table, the best performing strategy is the naive portfolio, where we rebalance to an equally weighted portfolio in each period. Using this strategy, we are able to generate a mean return of 9.83% which is significantly better than both of the other strategies. Moreover, the sharpe ratio suggest that the naive portfolio handles risk more efficiently than both of the other portfolios. It also becomes evident that transaction costs tend to kill returns, as both the naive and the portfolio with no short-selling strongly outperforms the transaction cost-adjusted portfolio. We thus see a negative correlation between turnover and returns, which underlines the importance of accounting for transaction costs when building a trading strategy. 


An argument for the out-of-sample experiment being "true" is that the observations used to estimate sigma (i.e., the training data) are not the one's used for the prediction. Thus, the forecast is made using a set of un-used observations, which, by definition, makes it an out-of-sample experiment. However, the experiments aim of testing how transaction costs affects the performance by rebalancing ex ante is made using some relatively unrealistic assumptions on the structure of transaction costs as we use a standardized cost of 200 bps for all transactions. As this does not reflect the true cost structure in the market, one could argue that the results fail to present a real representation of the effect of transaction costs. 

