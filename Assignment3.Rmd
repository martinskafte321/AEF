---
title: "Assignment3"
author: "Martin, Oliver and Fred"
date: "6/5/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, }
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Loading packages:
library(RSQLite)
library(tidyverse)
library(tidymodels) 
library(furrr) 
library(glmnet)
library(broom)
library(timetk)
library(scales)
library(lubridate)
library(scales)
library(frenchdata)
library(caret)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(mltools)
library(data.table)
library(vtable)
library(keras)
library(quadprog)
library(rmgarch)
library(xts)
```

### 1.1 Select all stocks from the CRSP universe with uninterrupted history such that you obtain a balanced panel of monthly (excess) returns for the entire time series from 1962 until 2020. Provide a brief summary of the sample: How many stocks does your investment universe consist of?

We start by loading the CRSP set.
```{r}
# Select all stocks from the CRSP universe
 tidy_finance <- dbConnect(SQLite(), "data/tidy_finance.sqlite",
                           extended_types = TRUE)

# Reading in crsp_monthly dataset
crsp_monthly <- tbl(tidy_finance, "crsp_monthly") %>%
  select(permno,month,ret_excess,industry) %>%
  collect() %>%
  mutate(month = as.Date(as.POSIXct.Date(month)))

remove(tidy_finance)
```

To keep the analysis simple we work with a balanced panel and exclude tickers, which are not traded for the full period. 
```{r}

# Clean data by removing stocks, which is not traded on every single trading day. 
data <- crsp_monthly %>%
  group_by(permno) %>%
  mutate(n=n()) %>%
  ungroup() %>%
  filter(n == max(n)) %>%
  select(-n)

remove(crsp_monthly)
```

We now turn to produce the summary statistics. From the plot, we find that the universe consists of 731 stocks.
```{r, fig.align = 'center'}
# Provide a brief summary of the sample:

table = data %>% 
  summarise(
        Variable = "Ret_excess",
        "N stocks"  = n_distinct(month),
        Mean = mean(ret_excess),
        SD = sd(ret_excess),
        Min = min(ret_excess),
        Max = max(ret_excess),
        Start = min(month),
        End = max(month)
         ) %>%
 mutate(Mean = format(Mean, digits = 2), SD = format(SD, digits = 2),
        Min = format(Min, digits = 3), Max = format(Max, digits = 3))

ggtable = ggtexttable(table, rows = NULL,
                       theme = ttheme("classic")) %>%
  tab_add_title(text = "Summary statistics")

plot = data %>% 
  select(permno, industry) %>%
  distinct() %>%
  group_by(industry) %>%
  summarise(n = n()) %>%
  ggplot(aes(x = industry, y = n)) +
  ggtitle("Number of stocks per industry") +
  geom_bar(stat="identity", width=.3, fill="tomato3") +
  labs(x="Industry class",
         y="Number of stocks") + 
    theme(axis.text.x = element_text(angle = 45, hjust=1))

# We can arrange the two plots to show them together in one graph. 
ggarrange(plot, ggtable, nrow = 2,
                   heights = c(1, 0.5)) 



remove(table,plot,ggtable, lineplot, figure)

```

### 2.1 Derive a closed-form solution for the mean-variance efficient portfolio ω∗t+1 based on the transaction cost specification above. Discuss if the assumption of transaction costs proportional to volatility makes sense.


### 2.2 Write a function that computes the optimal weights ω∗t+1 based on the inputs μ,Σ,γ,λ and the current weights ωt+. You can assume γ = 4 throughout the entire assignment.

To find the optimal weight we incorporate the compute_efficient_weights function from chapter 11.4 in (Scheuss et. al 2022), where mu denotes the historical mean return, sigma is the covariance matrix, gamma is the coefficient of risk aversion (4 by prespecification), lambda is the transaction costs and w_prev is the weight before rebalancing. The function calculates the efficient portfolio weight in general form, while allowing for transaction costs. The transaction costs are conditional on the holdings before reallocation. If lamda (i.e. the transaction costs) are zero, the computation resembles the standard mean-variance efficient framework (CAPM), where the aim is to maximize the sharpe ratio.
```{r}
# Function to compute optimal portfolio weights based on inputs. 

efficient_weights <- function(mu, sigma, gamma = 4, lambda, w_prev) {
  
  iota <- rep(1, ncol(sigma))
  
  sigma_adjusted <- sigma + lambda/gamma * diag(ncol(sigma))
  mu_adjusted <- mu + lambda*w_prev
  
  sigma_inv <- solve(sigma_adjusted)
  
  w_mvp <- sigma_inv %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  
  w_opt <- w_mvp + 1/gamma*(sigma_inv - (sigma_inv %*% iota %*% t(iota) %*% sigma_inv)/sum(sigma_inv)) %*% mu_adjusted
  return(as.vector(w_opt))
}

```


#### 2.3 Use the full sample of your data to compute sample mean returns and the sample variance covariance matrix.

First we calculate the return matrix, where the entire series of returns for each stock is obtained. From this it is possible to calculate the historical mean (mu) and covariance matrix (sigma).
```{r}
# We will need a a (T x N) matrix of returns to compute a matrix of returns: 
ret_matrix <- data %>%
  select(-industry) %>%
  pivot_wider(
    names_from = permno,
    values_from = ret_excess
  ) %>% 
  select(-month)


# using the returns matrix, we can compute the covariance matrix 
sigma <- cov(ret_matrix)
mu <- colMeans(ret_matrix) # extract the sample mean returns from the return matrix

remove(data)

```

Next we perform Ledoit-Wolf shrinkage on the variance-covariance matrix. This comes in handy, as the method serves as an effective way of overcoming issues related to var-covar matrices in large dimensions. The function is rather complex, why we refer the reader to chapter 9.2 in "Exercises for Advanced Empirical Finance: Topics and Data Science" by (Scheuss et. al 2022) for a deeper explanation.
```{r}
# Ledoit-Wolf estimation
compute_ledoit_wolf <- function(x) {
  # Computes Ledoit-Wolf shrinkage covariance estimator
  # This function generates the Ledoit-Wolf covariance estimator  as proposed in Ledoit, Wolf 2004 (Honey, I shrunk the sample covariance matrix.)
  # X is a (t x n) matrix of returns
  t <- nrow(x)
  n <- ncol(x)
  x <- apply(x, 2, function(x) if (is.numeric(x)) # demean x
    x - mean(x) else x)
  sample <- (1/t) * (t(x) %*% x)
  var <- diag(sample)
  sqrtvar <- sqrt(var)
  rBar <- (sum(sum(sample/(sqrtvar %*% t(sqrtvar)))) - n)/(n * (n - 1))
  prior <- rBar * sqrtvar %*% t(sqrtvar)
  diag(prior) <- var
  y <- x^2
  phiMat <- t(y) %*% y/t - 2 * (t(x) %*% x) * sample/t + sample^2
  phi <- sum(phiMat)

  repmat = function(X, m, n) {
    X <- as.matrix(X)
    mx = dim(X)[1]
    nx = dim(X)[2]
    matrix(t(matrix(X, mx, nx * n)), mx * m, nx * n, byrow = T)
  }

  term1 <- (t(x^3) %*% x)/t
  help <- t(x) %*% x/t
  helpDiag <- diag(help)
  term2 <- repmat(helpDiag, 1, n) * sample
  term3 <- help * repmat(var, 1, n)
  term4 <- repmat(var, 1, n) * sample
  thetaMat <- term1 - term2 - term3 + term4
  diag(thetaMat) <- 0
  rho <- sum(diag(phiMat)) + rBar * sum(sum(((1/sqrtvar) %*% t(sqrtvar)) * thetaMat))

  gamma <- sum(diag(t(sample - prior) %*% (sample - prior)))
  kappa <- (phi - rho)/gamma
  shrinkage <- max(0, min(1, kappa/t))
  if (is.nan(shrinkage))
    shrinkage <- 1
  sigma <- shrinkage * prior + (1 - shrinkage) * sample
  return(sigma)
}


```


#### 2.3 Suppose that the initial portfolio ωt+ is the naive, equal weighted allocation and the investor has a risk aversion of γ = 4. Compute the optimal portfolio ω∗t+1(λ) for different values of the transaction cost parameter λ.

To calculate the optimal portfolio with 1/N initial we will use the hard-coded weight parameter from the function, as this provides an initially equally-weighted portfolio. 
```{r, fig.width=8, fig.height=4}
# Creating the initial naive weight
naive_weights = 1/ncol(sigma) * rep(1,ncol(sigma))

w_efficient = efficient_weights(sigma = sigma,
                                   mu = mu,
                                   lambda = 0,
                                   w_prev = naive_weights)

transactions_costs <- expand_grid(lambda = 20 * qexp((1:99)/100)) %>%
  mutate(weights = map(.x = lambda,
                       ~efficient_weights(
                         sigma = sigma,
                         mu = mu,
                         lambda = .x / 10000,
                         w_prev = naive_weights
                         
                       )
                       ),
         concentration = map_dbl(weights, ~sum(abs(. - w_efficient)))
         )

transactions_costs %>% 
  ggplot(aes(x = lambda, y = concentration)) + 
  geom_line(color="tomato3") +
  labs(x = "Transaction cost parameter", 
       y = "Distance from efficient weights",
       title = "Distance from efficient portfolio weights for different transaction cost")

remove(transactions_costs, mu, sigma)

```
# What do you conclude about the effect of turnover penalization?
The higher the level of transaction costs, the closer will the optimal portfolio will be to the equally-weighted. Turnover penalization will make the investor reluctant towards making trades, as it is costly, and therefore choose appropriate portfolio-weights. This also makes sense as increasing transaction costs implies a higher demanded return - at some point, the costs just will not outweight the gains from the transactions, why investors will be reluctant to rebalance their portfolio. 


### 3. Implement a full-fledged portfolio backtesting strategy with transaction costs proportional to risk as above and suppose that λ = 200/10000 = 200bp. 

With rolling window forecasts, we recursively re-estimate the model and store the one ahead covariance forecasts. We implement the strategy in the following way. For the backtesting, we recompute optimal weight based purely on past available data. We use a window length of 150 periods which gives us 333 periods of performance. We test three strategies: A transaction cost-adjusted portfolio (MV (TC)), a mean-variance portfolio without short-selling, but with transaction costs (MV (No SS)) and an equally weighted naive portfolio (Naive). Initial weights are set as a equally weighted portfolio and we evaluate the portfolios through the mean returns nets of transaction costs, std. deviation of returns net of transaction costs, the sharpe ratio and the portfolio turnover.    

```{r}
# recomputing optimal weights

window_length <- 150
periods <-  nrow(ret_matrix) - window_length
lambda = 200/10000

performance_values <- matrix(NA,
                             nrow = periods,
                             ncol = 3) 

colnames(performance_values) = c("raw_return","turnover","net_return")


performance_values <- list("MV (TC)" = performance_values,
                           "MV (No SS)" = performance_values, 
                            "Naive" = performance_values)

w_prev_1 <- w_prev_2 <- w_prev_3 <- naive_weights # Initial weights of all portfolios

# Helper function to adjust weights due to returns changing
adjust_weights <- function(w, next_return){
  w_prev <- 1 + w * next_return
  as.numeric(w_prev / sum(as.vector(w_prev)))
}

# Helper function to calculate performance evaluation: compute realized returns net of transaction costs.
evaluate_performance <- function(w, w_previous, next_return, lambda){
  raw_return <- as.matrix(next_return) %*% w
  turnover <- sum(abs(w - w_previous))
  net_return <- raw_return - lambda * turnover
  c(raw_return, turnover, net_return)
}

```

With the helper functions, we are ready to perform the rolling-window estimation. This is done using a for loop through all the periods. Lastly, the performance is evaluated through the summarized key figures.
```{r}
# The following code chunk performs rolling-window estimation:

for (p in 1:periods) {
  
  returns_window <- ret_matrix[p : (p + window_length - 1), ] # window of return rows. window of rows and all columns
  one_ahead_return <- ret_matrix[p + window_length,] # Next periods return
  

  mu  <- colMeans(returns_window)
  sigma_lw <- compute_ledoit_wolf(returns_window)
  
  # Mean-variance transaction cost-adjusted portfolio:

  w_MV_TC <- efficient_weights(mu = mu,
                                    sigma = sigma_lw,
                                    lambda = lambda,
                                    w_prev = w_prev_1 # first naive, then adjusted.
                                    )
  
  performance_values[[1]][p,] <- evaluate_performance(w = w_MV_TC,
                                                      w_previous = w_prev_1,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda
                                                      )
  
  w_prev_1 = adjust_weights(w_MV_TC, one_ahead_return)
  

 # Mean-variance without short-selling portfolio (still with transaction costs):
  
  w_MV_NSS <- solve.QP(Dmat = 4 * sigma_lw,
                       dvec = mu,
                       Amat = cbind(1,diag(ncol(returns_window))),
                       bvec = c(1,rep(0, ncol(returns_window))),
                       meq = 1
                       )$solution
  
  
  performance_values[[2]][p,] <- evaluate_performance(w = w_MV_NSS,
                                                      w_previous = w_prev_2,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda
                                                      ) 
  
w_prev_2 = adjust_weights(w_MV_NSS, one_ahead_return)

  
  # Naive portfolio allocation:
  
  performance_values[[3]][p,] <- evaluate_performance(w = naive_weights,
                                                      w_previous = w_prev_3,
                                                      next_return = one_ahead_return,
                                                      lambda = lambda)
  
  w_prev_3 = adjust_weights(naive_weights, one_ahead_return)

}


performance <- lapply(performance_values, as_tibble) %>% 
  bind_rows(.id = "strategy")
  
performance %>%
  group_by(strategy) %>%
  summarise(
    Mean = 12 * mean(100 * raw_return),
    SD = 12 * sd(100 * raw_return),
    'Sharpe Ratio' = if_else(Mean > 0, Mean / SD, NA_real_),
    Turnover = mean(turnover)
  )
```
# Discuss the results: Which portfolio strategy performed best after adjusting for transaction costs and what are possible reasons for the differences in performance?
As presented in the above table, the best performing strategy is the naive portfolio, where we rebalance to an equally weighted portfolio in each period. Using this strategy, we are able to generate a mean return of 9.83% which is significantly better than both of the other strategies. Moreover, the sharpe ratio suggest that the naive portfolio handles risk more efficiently than both of the other portfolios. It also becomes evident that transaction costs tend to kill returns, as both the naive and the portfolio with no short-selling strongly outperforms the transaction cost-adjusted portfolio. We thus see a negative correlation between turnover and returns, which underlines the importance of accounting for transaction costs when building a trading strategy. 

# Discuss: would you consider the backtesting results as a “true” out-of-sample experiment? Why? Why not?
An argument for the out-of-sample experiment being "true" is that the observations used to estimate sigma (i.e., the training data) are not the one's used for the prediction. Thus, the forecast is made using a set of un-used observations, which, by definition, makes it an out-of-sample experiment. However, the experiments aim of testing how transaction costs affects the performance by rebalancing ex ante is made using some relatively unrealistic assumptions on the structure of transaction costs as we use a standardized cost of 200 bps for all transactions. As this does not reflect the true cost structure in the market, one could argue that the results fail to present a real representation of the effect of transaction costs. 

