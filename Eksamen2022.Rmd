---
title: "Exam2022"
author: "tfd199, lhb642, tgx333"
date: "6/11/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


```{r}
#Loading packages
library(readr)
library(RSQLite)
library(cvCovEst)
library(tidyverse)
library(tidymodels) 
library(furrr) 
library(glmnet)
library(broom)
library(timetk)
library(scales)
library(lubridate)
library(scales)
library(frenchdata)
library(caret)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(mltools)
library(data.table)
library(vtable)
library(keras)
library(quadprog)
library(rmgarch)
library(xts)

options(scipen=999)
```


#### 1
**Use the data until December 31, 2015 to estimate the parameters of the Fama French 3-factor model.**
We load in the csv file. First we filter the data so that we have until (and including) December 31st 2015. Next, we loop through each permno to perform the regression to estimate the parameters of the Fama-French 3 factor model. 
```{r}
#Loading in the CSV file
dataset <- read_csv("data/data_exam2022.csv") 

#Dataset for ex 1 and 2
data <- dataset %>%
  filter(month < "2016-01-01") 

#Dataset for ex 3
portfolio_data <- dataset %>%
  filter(month >= "2016-01-01")

#Estimation:
#Creating matrix to store alpha's and coefficient values
coefficients <- matrix(NA,
                             nrow = nrow(unique(data['permno'])),
                             ncol = 5) 

colnames(coefficients) = c("Permno","Alpha","beta_M","beta_smb","beta_hml")

models <- matrix(NA,
                             nrow = nrow(unique(data['permno'])),
                             ncol = 5) 

colnames(models) = c("Permno","Alpha","beta_M","beta_smb","beta_hml")

for (i in 1:nrow(unique(data['permno']))) { #Looping through permnos
  
  
  t =  unique(data['permno'])[i,]
  coefficients[i,2:5] <- lm(ret_excess ~ mkt_excess + smb + hml, 
     data = data %>% filter(permno == t[[1]] )) %>% coefficients()
  
  coefficients[i,1] <- t[[1]]
  
}

#Creating table to portray results
coefficients %>% kbl(caption = "Table 1: Summary of Fama-French 3 Factor model regression coefficients",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```
**Give a brief interpretation of the values.**
The above table shows the estimated alpha along with variable coefficients for all variables in the 3 factor model. Alpha can be interpreted as the intercept, i.e. the return in excess of the expected return, conditional on the variables included in the model. The beta values are the regression coefficients for the included variables and shows how much of the expected excess return of asset i that is explained by the market excess return, the small minus big and high minus low factor returns respectively.

**Derive the model-implied expected gross excess return and the model-implied variance covariance matrix.**
We use the estimated model for each stock to derive a full series of model-implied expected returns. When that is done, we take the mean of the series for each asset to arrive at a final model-implied expected gross excess return. For the variance covariance matrix, we simply use the cov() function on the return matrix used to find $\hat{\mu}^{FF}$.
```{r}
#Creating return matrix
ret_matrix <- matrix(NA,
                             nrow = nrow(data %>% filter(permno=='10026')),
                             ncol = 8) 

colnames(ret_matrix) = c("10026","10032","10044","10104","10200","10232","10252","10397")

for (m in 1:nrow(unique(data['permno']))) {
  
   l =  unique(data['permno'])[m,]
 ret_matrix[,m] <-  predict(lm(ret_excess ~ mkt_excess + smb + hml, 
     data = data %>% filter(permno == l[[1]] )), newdata = data %>% filter(permno==l[[1]]) %>% select(mkt_excess,smb,hml))
}

#Using the return matrix, we can compute the covariance matrix and sample mean
sigma <- linearShrinkLWEst(ret_matrix) #Using ledoit-wolf shrinkage to allow invertion of var-covar matrix
mu <- colMeans(ret_matrix)

```

```{r}
# Prepping dataframe for table generation
mu_tbl <- data.frame(t(mu)) 
names(mu_tbl) = substring(names(mu_tbl), 2)
mu_tbl['Permno'] <- 'Mu'
mu_tbl <- mu_tbl[ , c("Permno",    # Reorder data frame
                       names(mu_tbl)[names(mu_tbl) != "Permno"])]

#Creating table to show model-implied expected gross excess returns
mu_tbl %>% kbl(caption = "Table 2: Model-implied expected gross excess returns",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r}
#Creating table to show model-implied variance-covariance matrix
t(sigma) %>% kbl(caption = "Table 3: Model-implied variance-covariance matrix",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```



**What determines the correlation of the returns?**
In our model, all predictors (market excess, smb and hml returns) have the same value throughout the series across all permnos. Therefore, the only thing that determines the estimated excess returns, and thus the only thing that makes the estimated returns differ, are the weights assigned to each variable, i.e. the coefficients. Adding to this, the betas themselves are determined using OLS, where the aim is to fit the regression by minimizing the squared errors, i.e. the sum of the square differences between the observed and predicted values. Therefore, the beta value of a given firm depends on the performance of the firm's stock relative to the performance of all other stocks.



**Use your parameter estimates to report the computed implied mean-variance efficient portfolio weight for an investor with risk aversion $\gamma$ = 4**
As visible in table 4, we end up with weights that indicate an extremely aggressive trading strategy. This is most visible by the long position of 1180% in permno 10044 and the short position of 821% in permno 10104. As our weights sum to 1, i.e. 100%, we accept the weights, however, we want to note that we do not necessarily deem it a realistic strategy. 
```{r}
# Function to compute optimal portfolio weights:
efficient_weights <- function(mu, sigma, gamma = 4) {
  
  iota <- rep(1, ncol(sigma))
 
  sigma_inv <- solve(sigma) #Using Ledoit Wolf shrinkage, we can invert the matrix
  
  w_mvp <- sigma_inv %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  
  w_opt <- w_mvp + 1/gamma * (sigma_inv - 1 / sum(sigma_inv) * sigma_inv %*% iota %*% t(iota) %*% sigma_inv) %*% mu
  
  return(as.vector(w_opt))
}

#Computing efficient weights by calling function on our returns and var-covar matrix
w_efficient = efficient_weights(sigma = sigma,
                                   mu = mu
                                )

# Prepping table ensuring that the weights sum to 1 (100%)
w_efficient_tbl <- as.data.frame(w_efficient)
w_efficient_tbl['Permno'] <- c('10026', '10032', '10044', '10104', '10200', '10232', '10252', '10397')
w_efficient_tbl <- w_efficient_tbl[ , c("Permno",    # Reorder data frame
                       names(w_efficient_tbl)[names(w_efficient_tbl) != "Permno"])]
w_efficient_tbl[nrow(w_efficient_tbl) + 1,] = c("Sum",sum(w_efficient)) 
w_efficient_tbl <- w_efficient_tbl %>% mutate(w_efficient = as.numeric(w_efficient)) %>% pivot_wider(names_from = Permno, values_from = w_efficient)

#Generating table showing model-implied excess returns
w_efficient_tbl %>% kbl(caption = "Table 4: Stock weights estimated from implied excess returns (%)", digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```

#### 2

**What is the intuition behind the parameter vector $\theta$ and how do Brandt, Santa-Clara and Valkanov propose to estimate $\theta$?**
The authors model the portfolio weights of each asset as a function of the assets characteristics. The coefficients of this function are found by optimizing the investor’s average utility of the portfolio’s return over the sample period. The optimal weight in each stock is found by taking the sum of its market capitalization weight and an optimal deviation from that market cap weight which depends parametrically on the characteristics of the firms. The included characteristics are market cap, book-to-market ratio, lagged one-year returns of each firm. Moreover, investors are assumed to have constant relative risk aversion preferences. \

The method implies that the coefficients $\theta$ are held constant across assets over the entire time period. This implies that the portfolio weights depends solely on the characteristics and not its historic returns. Therefore, two stocks with similar characteristics associated with returns and risk should have similar weights even though their sample returns may differ. It is implicitly assumed that the aforementioned characteristics fully capture all aspects of the joint distribution of returns that are deemed relevant for building an optimal portfolio. This implies that the coefficients that maximizes the conditional expected utility of the investor at a given point in time are the same for all dates. Therefore, they also maximize the unconditional utility of the investors. \ 

\textbf{Estimating $\thea$:} In the paper, the coefficients, i.e. the maximum expected utility estimator, are estimated as a method of moments estimator from which the asymptotic covariance matrix can be estimated using the consistent estimator V. This approach allows the authors to test the model for certain problems, e.g. misspecified portfolio policy or with the implementation of different constraints which is allowed as long as the authors take the appropriate measures to estimate V (e.g., by using an autocorrelation-adjusted estimator of V when constraints are imposed). 


**Discuss the benefit of the parametrizing portfolio weights directly relative to the two-step procedure. What are potential disadvantages of direct weight parametrization?**
The authors argues that their method is beneficial for different reasons. It is computationally simple, easily modified and extended, produces sensible portfolio weights, and offers robust performance in and out of sample. Moreover, it is argued that parameterization leads to a tremendous reduction in dimensionality, and it escapes issues with imprecise coefficient estimates and overfitting. \

A disadvantage of the approach is the fact that characteristics are chosen arbitrarily, why one easily can assume that some information is not accounted for. Another disadvantage is that constraints on the portfolio weights (e.g. no-short selling constraints) implies that more complicated measures have to be implemented, as the weights will not automatically sum to one. Moreover, the critical assumptions regarding the characteristics fully capturing all aspects of the joint return distribution and the fact that coefficients are held constant for the entire period is somewhat unrealistic. As the authors write themselves: "While this is a convenient assumption, there is no obvious economic reason for the relation between firm characteristics and the joint distribution of returns to be time-invariant. In fact, there is substantial evidence that economic variables related to the business cycle forecast aggregate stock and bond returns".



**Report and discuss the values of \theta: Are the signs intuitive?**

In Table 4, the visible coefficients are the relative deviations from the benchmark naive portfolio due to characteristics market capitalization, book-to-market ratio and CAPM beta. These values optimize the objective function of maximizing the investors certainty equivalent. The values imply that expected utility for the investor decreases with size and beta, while it increases with bm. In other words, weight is assigned towards smaller companies, companies with lower book-to-market ratios and companies which stock move against the market. This is in line with past literature and findings in XXX, that focus is to be put on smaller companies and companies with lower book-to-market ratios.

```{r}
data = data %>%
  group_by(month) %>%
  mutate(
    n = n(),
  )

n_parameters <- 3
theta <- rep(1.5, n_parameters)
names(theta) <- c("size","bm","beta")

```

```{r}
evaluate_portfolio <- function(weights_crsp, gamma = 4) {
  
  evaluation <- weights_crsp %>%
    group_by(month) %>%
    summarise(
      return_tilt = weighted.mean(ret_excess, weight_tilt),
      return_benchmark = weighted.mean(ret_excess, weight_benchmark)
    ) %>%
    pivot_longer(-month, values_to = "portfolio_return", names_to = "model") %>%
    group_by(model) %>%
    summarise("Certainty equivalent" = mean(portfolio_return) - (gamma/2)*var(portfolio_return)
    ) %>%
    mutate(model = gsub("return_", "", model)) %>%
    pivot_longer(-model, names_to = "measure") %>%
    pivot_wider(names_from = model, values_from = value)

  return(evaluation)
}
```


```{r}
# Function to calculate weights:
compute_portfolio_weights <- function(theta,
                                      data) {
  data %>%
    group_by(month) %>%
    bind_cols(
      characteristic_tilt = data %>% ungroup %>%
        transmute(size = size / n,
                  bm = bm / n,
                  beta = beta / n) %>%
        as.matrix() %*% theta %>% as.numeric()
    ) %>%
    mutate(
      # Definition of benchmark weight
      weight_benchmark = 1 / n,
      # Parametric portfolio weights
      weight_tilt = weight_benchmark + characteristic_tilt,
      # Weights sum up to 1
      weight_tilt = weight_tilt / sum(weight_tilt) 
    ) %>%
    ungroup()
}


```


```{r}
compute_objective_function <- function(theta,
                                       data,
                                       objective_measure = "Certainty equivalent") {
  
  processed_data <- compute_portfolio_weights(
    theta,
    data
  )

  objective_function <- evaluate_portfolio(processed_data) %>%
    filter(measure == objective_measure) %>%
    pull(tilt)

  return(-objective_function)
}

optimal_theta <- optim(
  par = theta, 
  compute_objective_function,
  objective_measure = "Certainty equivalent",
  data = data
)

opt_theta <- optimal_theta$par

t(opt_theta) %>% kbl(caption = "Table 5: Optimal estimates of theta", digits = 3, col.names = c('Size', 'bm', 'beta')) %>%
  kable_classic(full_width = F, html_font = "Cambria")


```



**Compute and report the optimal weights $w^{PP}$ **

```{r}
#Calling function to find optimal weights based on parametres (theta)
optimal_weights <- compute_portfolio_weights(
    theta = opt_theta,
    data = data
  ) %>% group_by(permno) %>% filter(month == max(month)) %>% select(weight_tilt) 

#Generating table to portray optimal weights from pp strategy
optimal_weights %>% pivot_wider(names_from = permno, values_from = weight_tilt) %>% kbl(caption = "Table 6: Optimal weights in the last period based on theta and stock characteristics", digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```

### 3

**Compare the performance of the two portfolio weight vectors from Exercise 1 and Exercise 2 with the naive allocation and the efficient portfolio**
```{r}

#Weights for efficient portfolio
retmatrix_eff <- data %>% select(month, permno, ret_excess) %>% pivot_wider(
    names_from = permno,
    values_from = ret_excess
  ) %>% ungroup() %>% select(-month)

mu_eff <- colMeans(retmatrix_eff)
sigma_eff <- cov(retmatrix_eff)

weights_eff <- efficient_weights(mu_eff, sigma_eff)

#Weights from ex 2 and naive pf's
weights_all <- optimal_weights %>% mutate(weights_naive = 1/8)

#Appending weights from ex1
w_efficient = as.data.frame(w_efficient)
weights_all = cbind(weights_all, w_efficient)

#Appending weights from efficient portfolio
weights_eff <- as.data.frame(weights_eff)
weights_all = cbind(weights_all, weights_eff)


#Creating table with all weights
weights_all <- weights_all[, c(1, 4, 2, 3, 5)]
colnames(weights_all) <- c('permno','FF-3', 'PP', 'Naive', 'Efficient')
weights_all %>% kbl(caption = "Table 7: Weights from the 4 portfolios", digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


**Report the annualized Sharpe Ratio of the four strategies and discuss the differences.**
The naive portfolio allocation is undoubtedly the best performing strategy in this backtest. The strategy is preferable both in terms of giving the highest return and the lowest standard deviation (volatility) with values respectively at 12.5\% and 26.2\%, which gives a Sharpe ratio of 0.47. The second best performing strategy is the portfolio allocation based on efficient weights gaining a Sharpe ratio of 0.21. Further, the strategy of parametric portfolio choices from part 2 and the Fama-French 3-factor model from part 1 delivers respectively a Sharpe ratio of 0.067 and -0.081. 

The differences between on one side the efficient portfolio and naive, and on the other side the FF-3 and PP are quite significant. The FF-3 is performing really bad with a negative gross return, which can be supported by the fact that the portfolio weights are quite extreme in some cases. Table 4 shows that the strategy goes long in company '10044' by more than 11.800\% and short -8.209\%, which will determine some extreme positions. In a small tradable stock universe like this, extreme positions will have high importance, which is clear for the former mentioned position, since this gives a return of -111\% and -112\% for 10104 (this isn't reported in the table). This has a huge influence on the performance of the portfolio overall. Exposure towards single stocks can be diversified away, but with a small universe, it's difficult to implement a strategy like this. The problem of high weights likely lies in the variance-covariance matrix, which is very close to being non-invertiable due to the nature of the model (return estimates being linear dependent). 


```{r}
#Deriving annualized returns and std dev
evaluations <- portfolio_data %>% select(month, permno, ret_excess) %>% left_join(weights_all, by = 'permno') %>%
  pivot_longer(!c(month,permno,ret_excess), names_to = "portfolio", values_to = "w") %>% mutate(w_ret = ret_excess*w) %>% group_by(portfolio,permno) %>% summarise(ret = 12*mean(w_ret), sd = sqrt(12)*sd(w_ret))

#Creating table to evaluate performance through returns, std devs and sharpe ratios
evaluations %>% group_by(portfolio) %>% summarise(Return = sum(ret), SD = sum(sd)) %>% mutate('Sharpe Ratio' = Return/SD) %>% kbl(caption = "Table 8: Annualized returns, standard deviations and sharpe ratios of the 4 portfolios", digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria")


```

The differences between on one side the efficient portfolio and naive, and on the other side the FF-3 and PP are quite significant. The FF-3 is performing really bad with a negative gross return, which can be supported by the fact that the portfolio weights are quite extreme in some cases. Table 4 shows that the strategy goes long in company '10044' by more than 11.800\% and short -8.209\%, which will determine some extreme positions. In a small tradable stock universe like this, extreme positions will have high importance, which is clear for the former mentioned position, since this gives a return of -111\% and -112\% for 10104 (this isn't reported in the table). This has a huge influence on the performance of the portfolio overall. Exposure towards single stocks can be diversified away, but with a small universe, it's difficult to implement a strategy like this. The problem of high weights likely lies in the variance-covariance matrix, which is very close to being non-invertiable due to the nature of the model (return estimates being linear dependent). 

```{r}
#Generating figure to show individual return contributions for each stock in each portfolio
evaluations %>% ggplot(aes(
  x = as.factor(permno), y = ret, fill = portfolio
)) + geom_bar(stat="identity") +
labs(
    title = "Return generated per stock by various portfolios",
    x = "Permno",
    y = "Return generated",
    fill = "Portfolio"
  ) +
  scale_y_continuous(labels = scales::percent)
```

**Outline the potential issues of the back-testing strategy in light of implementation short- falls and estimation uncertainty.**

There exists several short-falls in the implementation of back-testing. To provide valid results through back-testing, all outside time-specific affecting factors has to be accounted for. Different time periods might not be suitable for comparisons, as circumstances that affect data points can change over time and results therefore can be affected by outside factors, such as booms and recessions or particular events. A general point is that what happened in the past cannot predict what will happen in the future, even though it can be argued that there lies information about the future in the past. \

In the calculation of returns, all values has to be based on weighted averages and cannot just be summed, as otherwise the aggregates would be affected by the length of the period considered (i.e. comparing return on a portfolio spanning over 10 years with one over 5 years). If considering just a few self-selected tickers when performing financial analysis, there is a risk of selection bias, which consists of non-random selection not being able to represent a gven population. Thus a backtest might not be representable for other periods. Further, we calculate returns on the weights estimated in $t_{end}$, meaning that time specific events (i.e. a very unusual event in the final month of 2015) might have influence on the estimation, and hence on the weights used going forward. Overfitting a model on a specific period might cause the model to not fit other periods.\

**Discuss your expectations regarding the performance of the four strategies for a high-dimensional asset universe and a realistic assessment of transaction costs.**

Generally, performance will increase in line with the asset space. This as portfolios can be better diversified over different assets, while returns will not decrease. Transaction costs will affect performance negatively, however, to different extents for the different portfolios.\

We expect the naive portfolio to do well also when considering a high-dimensional asset universe and transaction costs. Transaction costs will be comparatively low, as trades are strictly made when assets enter/exit the universe and when the relative weights between assets change as a result of last period differences in returns. However, comparing this to allocation changes associated with performance-tracking portfolio strategies, e.g. momentum strategies, the size of the re-balancing will be comparably low. The risk of low-performing stocks being a significant part of a portfolio decreases with the size of the asset universe, although this is also true for well-performing stocks. The general risk of the portfolio will decreases with a larger amount of assets as the contributing volatility of the individual asset become relatively smaller. The effort in maintenance is low. The portfolio will follow the idea that there is a higher performance of smaller companies, in comparison to a weighted market portfolio. \

For the Fama-French strategy, trades are made in line with how companies perform in relation to the considered Fama-French factors. This can change relatively frequently, as firms' returns may fluctuate from month to month. Moreover, this can cause returns for the factor returns to vary. In times of financial distress, this will be more prominent for e.g. the smb factor as small-cap stocks tend to perform significantly worse than their large-cap counterparts during recessions why the factor returns will suffer from high volatility ceteris paribus. Given this, a larger asset universe will imply more transactions, which further implies relatively large transaction costs. Therefore, a large asset universe may decrease the performance of this type of portfolio because large transaction costs will kill returns. \

When the asset universe increase, the efficient portfolio will move closer towards the market portfolio. Transaction costs will decrease with an increase in the number of assets in the portfolio, as changes in returns will affect both factors of the Sharpe ratio, returns and risk. Therefore, we assess the effect of transaction costs as somewhat moderate, as they will be larger than those of the naive portfolio but smaller than those of the three-factor portfolio. This has been further supported by the performance we saw in the third mandatory assignment of this course. \

The portfolio based on parametriced characteristics has its strength in large universes that requires extensive calculations, as it reduces dimensionality.It is also argued that the method escapes some estimation uncertainty, through its different approach. In terms of transaction costs, the model will be re balanced in line with changes in companies' relation to the characteristics size, bm CAPM beta. It can be argued that weights will change relatively little over time, as companies will stay fairly stable over time, in relation to the different characteristics. Transaction costs can therefore be interpreted as relatively low. With a larger asset space, the risk in returns will decrease. \






