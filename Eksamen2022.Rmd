---
title: "Exam2022"
author: "tfd199, lhb642, tgx333"
date: "6/11/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, message=FALSE,warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


```{r}
library(readr)
library(RSQLite)
library(cvCovEst)
library(tidyverse)
library(tidymodels) 
library(furrr) 
library(glmnet)
library(broom)
library(timetk)
library(scales)
library(lubridate)
library(scales)
library(frenchdata)
library(caret)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(mltools)
library(data.table)
library(vtable)
library(keras)
library(quadprog)
library(rmgarch)
library(xts)

options(scipen=999)
```


#### 1
**Use the data until December 31, 2015 to estimate the parameters of the Fama French 3-factor model.**
We load in the csv file. First we filter the data so that we have until (and including) December 31st 2015. Next, we loop through each permno to perform the regression to estimate the parameters of the Fama-French 3 factor model. 
```{r}
#Loading in the CSV file
dataset <- read_csv("data/data_exam2022.csv") 

data <- dataset %>%
  filter(month < "2016-01-01") 

portfolio_data <- dataset %>%
  filter(month >= "2016-01-01")

#Estimation:
#Creating matrix to store coefficient values
coefficients <- matrix(NA,
                             nrow = nrow(unique(data['permno'])),
                             ncol = 5) 

colnames(coefficients) = c("Permno","Alpha","beta_M","beta_smb","beta_hml")

models <- matrix(NA,
                             nrow = nrow(unique(data['permno'])),
                             ncol = 5) 

colnames(models) = c("Permno","Alpha","beta_M","beta_smb","beta_hml")

for (i in 1:nrow(unique(data['permno']))) { #Looping through permnos
  
  
  t =  unique(data['permno'])[i,]
  coefficients[i,2:5] <- lm(ret_excess ~ mkt_excess + smb + hml, 
     data = data %>% filter(permno == t[[1]] )) %>% coefficients()
  
  coefficients[i,1] <- t[[1]]
  
}

#Creating table to portray results
coefficients %>% kbl(caption = "Table 1: Summary of Fama-French 3 Factor model regression coefficients",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```
**Give a brief interpretation of the values.**
The above table shows the estimated alpha along with variable coefficients for all variables in the 3 factor model. Alpha can be interpreted as the intercept, i.e. the return in excess of the expected return, conditional on the variables included in the model. The beta values are the regression coefficients for the included variables and shows how much of the expected excess return of asset i that is explained by the market excess return, the small minus big and high minus low factor returns respectively.

**Derive the model-implied expected gross excess return and the model-implied variance covariance matrix.**
We use the estimated model for each stock to derive a full series of model-implied expected returns. When that is done, we take the mean of the series for each asset to arrive at a final model-implied expected gross excess return. For the variance covariance matrix, we simply use the cov() function on the return matrix used to find $\hat{\mu}^{FF}$.
```{r}

ret_matrix <- matrix(NA,
                             nrow = nrow(data %>% filter(permno=='10026')),
                             ncol = 8) 

colnames(ret_matrix) = c("10026","10032","10044","10104","10200","10232","10252","10397")

for (m in 1:nrow(unique(data['permno']))) {
  
   l =  unique(data['permno'])[m,]
 ret_matrix[,m] <-  predict(lm(ret_excess ~ mkt_excess + smb + hml, 
     data = data %>% filter(permno == l[[1]] )), newdata = data %>% filter(permno==l[[1]]) %>% select(mkt_excess,smb,hml))
}

# using the returns matrix, we can compute the covariance matrix and sample mean
sigma <- linearShrinkLWEst(ret_matrix)
mu <- colMeans(ret_matrix)

```

```{r}
# Prepping dataframe for table generation
mu_tbl <- data.frame(t(mu)) 
names(mu_tbl) = substring(names(mu_tbl), 2)
mu_tbl['Permno'] <- 'Mu'
mu_tbl <- mu_tbl[ , c("Permno",    # Reorder data frame
                       names(mu_tbl)[names(mu_tbl) != "Permno"])]

mu_tbl %>% kbl(caption = "Table 2: Model-implied expected gross excess returns",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r}
t(sigma) %>% kbl(caption = "Table 3: Model-implied variance-covariance matrix",digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```



**What determines the correlation of the returns?**
In our model, all predictors (market excess, smb and hml returns) have the same value throughout the series across all permnos. Therefore, the only thing that determines the estimated excess returns, and thus the only thing that makes the estimated returns differ, are the weights assigned to each variable, i.e. the coefficients. Adding to this, the betas themselves are determined using OLS, where the aim is to fit the regression by minimizing the squared errors, i.e. the sum of the square differences between the observed and predicted values. Therefore, the beta value of a given firm depends on the performance of the firm's stock relative to the performance of all other stocks.



**Use your parameter estimates to report the computed implied mean-variance efficient portfolio weight for an investor with risk aversion $\gamma$ = 4**
As visible in table 4, we end up with weights that indicate an extremely aggressive trading strategy. This is most visible by the long position of 1180% in permno 10044 and the short position of 821% in permno 10104. As our weights sum to 1, i.e. 100%, we accept the weights, however, we want to note that we do not necessarily deem it a realistic strategy. 
```{r}
# Function to compute optimal portfolio weights:
efficient_weights <- function(mu, sigma, gamma = 4) {
  
  iota <- rep(1, ncol(sigma))
 
  sigma_inv <- solve(sigma) #Using Ledoit Wolf shrinkage, we can invert the matrix
  
  w_mvp <- sigma_inv %*% iota
  w_mvp <- as.vector(w_mvp / sum(w_mvp))
  
  w_opt <- w_mvp + 1/gamma * (sigma_inv - 1 / sum(sigma_inv) * sigma_inv %*% iota %*% t(iota) %*% sigma_inv) %*% mu
  
  return(as.vector(w_opt))
}

w_efficient = efficient_weights(sigma = sigma,
                                   mu = mu
                                )

# Prepping table ensuring that the weights sum to 1 (100%)
w_efficient_tbl <- as.data.frame(w_efficient)
w_efficient_tbl['Permno'] <- c('10026', '10032', '10044', '10104', '10200', '10232', '10252', '10397')
w_efficient_tbl <- w_efficient_tbl[ , c("Permno",    # Reorder data frame
                       names(w_efficient_tbl)[names(w_efficient_tbl) != "Permno"])]
w_efficient_tbl[nrow(w_efficient_tbl) + 1,] = c("Sum",sum(w_efficient)) 
w_efficient_tbl <- w_efficient_tbl %>% mutate(w_efficient = as.numeric(w_efficient)) %>% pivot_wider(names_from = Permno, values_from = w_efficient)

w_efficient_tbl %>% kbl(caption = "Table 4: Stock weights estimated from implied excess returns", digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```

#### 2

**What is the intuition behind the parameter vector $\theta$ and how do Brandt, Santa-Clara and Valkanov propose to estimate $\theta$?**
The authors model the portfolio weights of each asset as a function of the assets characteristics. The coefficients of this function are found by optimizing the investor’s average utility of the portfolio’s return over the sample period. The optimal weight in each stock is found by taking the sum of its market capitalization weight and an optimal deviation from that market cap weight which depends parametrically on the characteristics of the firms. The included characteristics are market cap, book-to-market ratio, lagged one-year returns of each firm. Moreover, investors are assumed to have constant relative risk aversion preferences. \

The method implies that the coefficients $\theta$ are held constant across assets over the entire time period. This implies that the portfolio weights depends solely on the characteristics and not its historic returns. Therefore, two stocks with similar characteristics associated with returns and risk should have similar weights even though their sample returns may differ. It is implicitly assumed that the aforementioned characteristics fully capture all aspects of the joint distribution of returns that are deemed relevant for building an optimal portfolio. This implies that the coefficients that maximizes the conditional expected utility of the investor at a given point in time are the same for all dates. Therefore, they also maximize the unconditional utility of the investors. \ 

\textbf{Estimating $\thea$:} In the paper, the coefficients, i.e. the maximum expected utility estimator, are estimated as a method of moments estimator from which the asymptotic covariance matrix can be estimated using the consistent estimator V. This approach allows the authors to test the model for certain problems, e.g. misspecified portfolio policy or with the implementation of different constraints which is allowed as long as the authors take the appropriate measures to estimate V (e.g., by using an autocorrelation-adjusted estimator of V when constraints are imposed). 


**Discuss the benefit of the parametrizing portfolio weights directly relative to the two-step procedure. What are potential disadvantages of direct weight parametrization?**
The authors argues that their method is beneficial for different reasons. It is computationally simple, easily modified and extended, produces sensible portfolio weights, and offers robust performance in and out of sample. Moreover, it is argued that parameterization leads to a tremendous reduction in dimensionality, and it escapes issues with imprecise coefficient estimates and overfitting. \

A disadvantage of the approach is the fact that characteristics are chosen arbitrarily, why one easily can assume that some information is not accounted for. Another disadvantage is that constraints on the portfolio weights (e.g. no-short selling constraints) implies that more complicated measures have to be implemented, as the weights will not automatically sum to one. Moreover, the critical assumptions regarding the characteristics fully capturing all aspects of the joint return distribution and the fact that coefficients are held constant for the entire period is somewhat unrealistic. As the authors write themselves: "While this is a convenient assumption, there is no obvious economic reason for the relation between firm characteristics and the joint distribution of returns to be time-invariant. In fact, there is substantial evidence that economic variables related to the business cycle forecast aggregate stock and bond returns".



**Report and discuss the values of \theta: Are the signs intuitive?**

In Table 4, the visible coefficients are the relative deviations from the benchmark naive portfolio due to characteristics market capitalization, book-to-market ratio and CAPM beta. These values optimize the objective function of maximizing the investors certainty equivalent. The values imply that expected utility for the investor decreases with size and beta, while it increases with bm. In other words, weight is assigned towards smaller companies, companies with lower book-to-market ratios and companies which stock move against the market. This is in line with past literature and findings in XXX, that focus is to be put on smaller companies and companies with lower book-to-market ratios.

```{r}
data = data %>%
  group_by(month) %>%
  mutate(
    n = n(),
  )

n_parameters <- 3
theta <- rep(1.5, n_parameters)
names(theta) <- c("size","bm","beta")

```

```{r}
evaluate_portfolio <- function(weights_crsp, gamma = 4) {
  
  evaluation <- weights_crsp %>%
    group_by(month) %>%
    summarise(
      return_tilt = weighted.mean(ret_excess, weight_tilt),
      return_benchmark = weighted.mean(ret_excess, weight_benchmark)
    ) %>%
    pivot_longer(-month, values_to = "portfolio_return", names_to = "model") %>%
    group_by(model) %>%
    summarise("Certainty equivalent" = mean(portfolio_return) - (gamma/2)*var(portfolio_return)
    ) %>%
    mutate(model = gsub("return_", "", model)) %>%
    pivot_longer(-model, names_to = "measure") %>%
    pivot_wider(names_from = model, values_from = value)

  return(evaluation)
}
```


```{r}
# Function to calculate weights:
compute_portfolio_weights <- function(theta,
                                      data) {
  data %>%
    group_by(month) %>%
    bind_cols(
      characteristic_tilt = data %>% ungroup %>%
        transmute(size = size / n,
                  bm = bm / n,
                  beta = beta / n) %>%
        as.matrix() %*% theta %>% as.numeric()
    ) %>%
    mutate(
      # Definition of benchmark weight
      weight_benchmark = 1 / n,
      # Parametric portfolio weights
      weight_tilt = weight_benchmark + characteristic_tilt,
      # Weights sum up to 1
      weight_tilt = weight_tilt / sum(weight_tilt) 
    ) %>%
    ungroup()
}


```


```{r}
compute_objective_function <- function(theta,
                                       data,
                                       objective_measure = "Certainty equivalent") {
  
  processed_data <- compute_portfolio_weights(
    theta,
    data
  )

  objective_function <- evaluate_portfolio(processed_data) %>%
    filter(measure == objective_measure) %>%
    pull(tilt)

  return(-objective_function)
}

optimal_theta <- optim(
  par = theta, 
  compute_objective_function,
  objective_measure = "Certainty equivalent",
  data = data
)

opt_theta <- optimal_theta$par

t(opt_theta) %>% kbl(caption = "Table 5: Optimal estimates of theta", digits = 3, col.names = c('Size', 'bm', 'beta')) %>%
  kable_classic(full_width = F, html_font = "Cambria")


```



**Compute and report the optimal weights $w^{PP}$ **

```{r}
optimal_weights <- compute_portfolio_weights(
    theta = opt_theta,
    data = data
  ) %>% group_by(permno) %>% filter(month == max(month)) %>% select(weight_tilt) 

optimal_weights %>% pivot_wider(names_from = permno, values_from = weight_tilt) %>% kbl(caption = "Table 6: Optimal weights in the last period based on theta and stock characteristics", digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```

### 3

**Compare the performance of the two portfolio weight vectors from Exercise 1 and Exercise 2 with the naive allocation and the efficient portfolio**
```{r}

#Weights for efficient portfolio
retmatrix_eff <- data %>% select(month, permno, ret_excess) %>% pivot_wider(
    names_from = permno,
    values_from = ret_excess
  ) %>% ungroup() %>% select(-month)

mu_eff <- colMeans(retmatrix_eff)
sigma_eff <- cov(retmatrix_eff)

weights_eff <- efficient_weights(mu_eff, sigma_eff)

#Weights from ex 2 and naive pf's
weights_all <- optimal_weights %>% mutate(weights_naive = 1/8)

#appending weights from ex1
w_efficient = as.data.frame(w_efficient)
weights_all = cbind(weights_all, w_efficient)

#appending weights from efficient portfolio
weights_eff <- as.data.frame(weights_eff)
weights_all = cbind(weights_all, weights_eff)


#Creating table with all weights
weights_all <- weights_all[, c(1, 4, 2, 3, 5)]
colnames(weights_all) <- c('permno','FF-3', 'PP', 'Naive', 'Efficient')
weights_all %>% kbl(caption = "Table 7: Weights from the 4 portfolios", digits = 4) %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


**Report the annualized Sharpe Ratio of the four strategies and discuss the differences.**
The naive portfolio allocation is undoubtedly the best performing strategy in this backtest. The strategy is preferable both in terms of giving the highest return and the lowest standard deviation (volatility) with values respectively at 12.5\% and 26.2\%, which gives a Sharpe ratio of 0.47. The second best performing strategy is the portfolio allocation based on efficient weights gaining a Sharpe ratio of 0.21. Further, the strategy of parametric portfolio choices from part 2 and the Fama-French 3-factor model from part 1 delivers respectively a Sharpe ratio of 0.067 and -0.081. 

The differences between on one side the efficient portfolio and naive, and on the other side the FF-3 and PP are quite significant. The FF-3 is performing really bad with a negative gross return, which can be supported by the fact that the portfolio weights are quite extreme in some cases. Table 4 shows that the strategy goes long in company '10044' by more than 11.800\% and short -8.209\%, which will determine some extreme positions. In a small tradable stock universe like this, extreme positions will have high importance, which is clear for the former mentioned position, since this gives a return of -111\% and -112\% for 10104 (this isn't reported in the table). This has a huge influence on the performance of the portfolio overall. Exposure towards single stocks can be diversified away, but with a small universe, it's difficult to implement a strategy like this. The problem of high weights likely lies in the variance-covariance matrix, which is very close to being non-invertiable due to the nature of the model (return estimates being linear dependent). 


```{r}
evaluations <- portfolio_data %>% select(month, permno, ret_excess) %>% left_join(weights_all, by = 'permno') %>%
  pivot_longer(!c(month,permno,ret_excess), names_to = "portfolio", values_to = "w") %>% mutate(w_ret = ret_excess*w) %>% group_by(portfolio,permno) %>% summarise(ret = 12*mean(w_ret), sd = sqrt(12)*sd(w_ret))

evaluations %>% group_by(portfolio) %>% summarise(Return = sum(ret), SD = sum(sd)) %>% mutate('Sharpe Ratio' = Return/SD) %>% kbl(caption = "Table 8: Annualized returns, standard deviations and sharpe ratios of the 4 portfolios", digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria")


```

The differences between on one side the efficient portfolio and naive, and on the other side the FF-3 and PP are quite significant. The FF-3 is performing really bad with a negative gross return, which can be supported by the fact that the portfolio weights are quite extreme in some cases. Table 4 shows that the strategy goes long in company '10044' by more than 11.800\% and short -8.209\%, which will determine some extreme positions. In a small tradable stock universe like this, extreme positions will have high importance, which is clear for the former mentioned position, since this gives a return of -111\% and -112\% for 10104 (this isn't reported in the table). This has a huge influence on the performance of the portfolio overall. Exposure towards single stocks can be diversified away, but with a small universe, it's difficult to implement a strategy like this. The problem of high weights likely lies in the variance-covariance matrix, which is very close to being non-invertiable due to the nature of the model (return estimates being linear dependent). 

```{r}
evaluations %>% ggplot(aes(
  x = as.factor(permno), y = ret, fill = portfolio
)) + geom_bar(stat="identity") +
labs(
    title = "Return generated per stock by various portfolios",
    x = "Permno",
    y = "Return generated",
    fill = "Portfolio"
  ) +
  scale_y_continuous(labels = scales::percent)
```

**Outline the potential issues of the back-testing strategy in light of implementation short- falls and estimation uncertainty.**

There exists several short-falls in the implementation of back-testing. To provide valid results through back-testing, all outside time-specific affecting factors has to be accounted for. Different time periods might not be suitable for comparisons, as circumstances that affect data points can change over time and results therefore can be affected by outside factors, such as booms and recessions or particular events. A general point is that what happened in the past cannot predict what will happen in the future, even though it can be argued that there lies information about the future in the past. \

In the calculation of values, all values has to be based on weighted averages and cannot be summed. This as values otherwise would be affected by the length of the dataset considered, and different periods would not be comparable. If considering just a few tickers when performing financial back-testing, there is a risk of selection bias. It is important to not base a strategy on a back-test, as data might not be representable for other periods, and therefore not back-test until all research is conducted and the model is fullt specified. Overfitting a model on the data might cause the model to not fit other data. When back-testing, it is also important to ensure that results are re-producable.\

**Discuss your expectations regarding the performance of the four strategies for a high-dimensional asset universe and a realistic assessment of transaction costs.**

We expect the naive portfolio to do well also when considering a high-dimensional asset universe and transaction costs. Transaction costs will be comparatively low, as trades are made only when assets enter and exit the universe. The risk of low-performing stocks being a significant part of a portfolio decreases with the size of the asset universe, although this is also true for well-performing stocks. The general risk of the portfolio will decreases with a larger amount of assets. The effort in maintenance is low. The portfolio will follow the idea that there is a higher performance of smaller companies, in comparison to a weighted market portfolio. \

A portfolio with the Fama-French factors can also be considered relatively cheap in terms of transaction costs. Trades are to be made in line with how companies develop, in relation to the considered Fama-French factors. In line with a larger asset universe, it should be easier for the model to properly weight a portfolio in terms of the included factors. \

The efficient portfolio will become more diversified with a broader market portfolio, and risk will therefore decrease. The portfolio escapes some transaction costs, as parts of the portfolio is invested in the risk-free asset. \

The portfolio based on parametriced characteristics has its strength in large universes that requires extensive calculations, as it reduces dimensionality. It is also argued to escape some estimation uncertainty. In terms of transaction costs, the model will be re balanced in line with changes in companies' relation to the characteristics beta, size and CAPM beta. It can be argued that companies' are fairly stable in terms of the characteristics and that transaction costs therefore are kept on a reasonable level.
